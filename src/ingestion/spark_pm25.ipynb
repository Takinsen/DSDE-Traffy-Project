{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8986b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/d:/CEDT/Course/Data_Science/project/src/data/bangkok_traffy.csv. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     29\u001b[39m spark.conf.set(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.ansi.enabled\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# LOAD DATA\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m df_traffy = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/bangkok_traffy.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m df_geo = spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33m../data/thailand_geography.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ... (Load data code remains the same) ...\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Clean Geo Data first\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/d:/CEDT/Course/Data_Science/project/src/data/bangkok_traffy.csv. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import DoubleType # Use Double, not Float\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, split, to_timestamp, coalesce, \n",
    "                                   regexp_replace, trim, regexp_extract, length, \n",
    "                                   substring, lit)\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# --- ADD THIS BLOCK ---\n",
    "# Allow the Security Manager for Java 21+\n",
    "os.environ[\"JDK_JAVA_OPTIONS\"] = \"-Djava.security.manager=allow\"\n",
    "# ----------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Traffy\").getOrCreate()\n",
    "\n",
    "#Turn off ANSI mode to allow silent null coercion\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------------------------------------\n",
    "df_traffy = spark.read.csv(\"../data/bangkok_traffy.csv\", header=True, inferSchema=False)\n",
    "df_geo = spark.read.csv(\"../data/thailand_geography.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ... (Load data code remains the same) ...\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: IMPROVED COORDINATE IMPUTATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Clean Geo Data first\n",
    "df_geo_clean = df_geo.select(\n",
    "    trim(col(\"district\")).alias(\"geo_district\"), \n",
    "    col(\"latitude\").alias(\"geo_lat\"),       \n",
    "    col(\"longitude\").alias(\"geo_long\")      \n",
    ")\n",
    "\n",
    "# Parse Traffy Data\n",
    "# We replace square brackets just in case the format is [10.1, 20.2]\n",
    "df_parsed = df_traffy.withColumn(\"coords_clean\", regexp_replace(col(\"coords\"), r\"[\\[\\]]\", \"\")) \\\n",
    "                     .withColumn(\"lat_str\", split(col(\"coords_clean\"), \",\").getItem(0)) \\\n",
    "                     .withColumn(\"long_str\", split(col(\"coords_clean\"), \",\").getItem(1)) \\\n",
    "                     .withColumn(\"raw_lat\", regexp_extract(col(\"lat_str\"), r\"([0-9]+\\.?[0-9]*)\", 1).cast(DoubleType())) \\\n",
    "                     .withColumn(\"raw_long\", regexp_extract(col(\"long_str\"), r\"([0-9]+\\.?[0-9]*)\", 1).cast(DoubleType()))\n",
    "\n",
    "# Debug: Print schema to ensure types are correct\n",
    "df_parsed.printSchema()\n",
    "\n",
    "# Join\n",
    "# Note: Ensure both sides are trimmed during the join condition\n",
    "df_joined = df_parsed.join(df_geo_clean, \n",
    "                           trim(df_parsed.district) == df_geo_clean.geo_district, \n",
    "                           \"left\")\n",
    "\n",
    "df_loc_fixed = df_joined.withColumn(\"final_lat\", coalesce(col(\"raw_lat\"), col(\"geo_lat\"))) \\\n",
    "                        .withColumn(\"final_long\", coalesce(col(\"raw_long\"), col(\"geo_long\")))\n",
    "\n",
    "# Check how many Nulls exist now\n",
    "print(\"Rows with NULL latitude:\", df_loc_fixed.filter(col(\"final_lat\").isNull()).count())\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: TEXT CLEANING\n",
    "# ---------------------------------------------------------\n",
    "df_clean_text = df_loc_fixed.withColumn(\"clean_comment\", \n",
    "    regexp_replace(col(\"comment\"), r\"[\\n\\r\\t]\", \" \")) \\\n",
    "    .withColumn(\"clean_comment\", regexp_replace(col(\"clean_comment\"), r\"\\s+\", \" \")) \\\n",
    "    .withColumn(\"clean_comment\", trim(col(\"clean_comment\")))\n",
    "\n",
    "# Filter out short comments\n",
    "df_ready = df_clean_text.filter(length(col(\"clean_comment\")) > 3)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: TEMPORAL FEATURES\n",
    "# ---------------------------------------------------------\n",
    "df_final = df_ready.withColumn(\"timestamp_dt\", \n",
    "                               to_timestamp(substring(col(\"timestamp\"), 1, 19), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                   .withColumn(\"last_activity_dt\", \n",
    "                               to_timestamp(substring(col(\"last_activity\"), 1, 19), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: FINAL EXPORT\n",
    "# ---------------------------------------------------------\n",
    "output_df = df_final.select(\n",
    "    \"ticket_id\",\n",
    "    \"type\", \n",
    "    \"clean_comment\",        \n",
    "    \"final_lat\",          \n",
    "    \"final_long\",           \n",
    "    \"district\",             \n",
    "    \"timestamp_dt\",\n",
    "    \"state\",\n",
    "    \"star\"\n",
    ")\n",
    "\n",
    "# Export\n",
    "print(\"Exporting data...\")\n",
    "output_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"traffy_data_powerBI\")\n",
    "print(\"Process Complete! Files saved to 'traffy_data_powerBI'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
